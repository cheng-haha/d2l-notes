{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn \n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tensor.repeat和torch.repeat_interleave的对比\n",
    "1. 实际上tensor.repeat是对原来的tensor进行修改得到一个返回变量，不是原地修改！！！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones( 2 , 2 )\n",
    "a.repeat( 1 , 1 , 1 )\n",
    "print( a.shape )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 基本用法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "a = a.repeat( 2  , 1 ,1 )\n",
    "print( a.shape  )#自动匹配轴数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. torch.repeat_interleave()的用法:一般来说第一个参数就是需要复制的tensor，然后第二个参数是要复制的参数数量。然后返回的是展平的向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 2, 2, 3, 3])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor( [ 1 , 2 , 3 ] )\n",
    "torch.repeat_interleave( x  , 2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_softmax( X , valid_lens  ):\n",
    "    \"\"\"通过在最后一个轴上遮蔽元素来执行 softmax 操作，就是将数据X根据valid_lens的每一个元素为长度进行遮蔽\n",
    "        请参看加性注意力中的语句有对为什么要使用遮蔽进行解释\"\"\"\n",
    "    if valid_lens == None:#对最后一个维度进行softmax\n",
    "        return nn.functional.softmax( X , dim= -1 )\n",
    "    else:\n",
    "        shape = X.shape\n",
    "        if valid_lens.dim() == 1:#实际上，valid_lens每个元素对应的是一个样本的遮蔽，也就是说valid_lens必须是batch_size\n",
    "            #的长度，但是如果说一个样本用一个二维张量去表示的话，就需要扩大valid_lens的长度，就是将valid_lens的每个元素*shape[ 1 ]\n",
    "            #这样得到的valid_lens经过广播后能对每个样本进行遮蔽。但是请注意：因为我们只是简单将每个元素进行了复制，所以我们是对每个\n",
    "            #样本的每个num_steps进行相同尺度的遮蔽。\n",
    "            #比如说这个样本是[[1,1],[1,1]]，valid_lens[1]---经过repeat---->[1 ,1 ]---经过sequence_mask----得到样本为[[1,0], [1,0]]\n",
    "                valid_lens = torch.repeat_interleave( valid_lens , shape[ 1 ] )\n",
    "                #如果没有指定第三个参数---轴的话，那么返回的是展开的一阶张量\n",
    "        else:#如果想进行对每个num_steps进行不同尺度的遮蔽，就传入二阶张量，那么请注意：传入的二阶张量必须是batch_size*num_steps的形状\n",
    "            valid_lens = valid_lens.reshape( -1 )#这里主要是传入形状大于1的tensor,  就直接将其展开\n",
    "        #保留最后的轴，基本的意思就是保留特征维度，这里主要是要将X进行二维化，这样可能方便对数据进行遮蔽\n",
    "        X = d2l.sequence_mask( X.reshape( -1 , shape[ -1 ] ) , valid_lens , value= -1e6 )\n",
    "        #将X进行reshape\n",
    "        return nn.functional.softmax( X.reshape( shape ) , dim= -1 )        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 torch.Size([2, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones( 2, 1 , 2 )\n",
    "y = torch.ones( 2 , 2 , 1 )#还是用到了广播机制\n",
    "print( (x + y).dim() , ( x + y ).shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.4897, 0.5103, 0.0000, 0.0000],\n",
       "         [0.3401, 0.6599, 0.0000, 0.0000]],\n",
       "\n",
       "        [[0.3752, 0.3940, 0.2308, 0.0000],\n",
       "         [0.2726, 0.2890, 0.4384, 0.0000]]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_softmax( torch.rand(2, 2, 4) , torch.tensor( [2, 3] ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.3622, 0.4338, 0.2040, 0.0000]],\n",
       "\n",
       "        [[0.4918, 0.5082, 0.0000, 0.0000],\n",
       "         [0.2126, 0.3161, 0.1996, 0.2717]]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_softmax(torch.rand(2, 2, 4), torch.tensor([[1, 3], [2, 4]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加性注意力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdditiveAttention( nn.Module ):\n",
    "    '''加性注意力，这里对遮蔽进行解释：我们会在qk的乘积得到的矩阵进行遮蔽，一般来说我们是对单个批次的每一行进行遮蔽，这样的意思是将单个时间步与相对的时间步的注意力分数进行遮蔽。\n",
    "        我们由此得到的注意力分数与values进行相乘，可以得到相对的注意力机制'''\n",
    "    def __init__( self , key_size  , query_size , num_hiddens , dropout , **kwargs ):\n",
    "        super( ).__init__( **kwargs )\n",
    "        self.W_k = nn.Linear( key_size , num_hiddens , bias= False )\n",
    "        self.W_q = nn.Linear( query_size , num_hiddens , bias= False )\n",
    "        self.W_v = nn.Linear( num_hiddens , 1 , bias= False )\n",
    "        self.dropout = nn.Dropout( dropout )\n",
    "    #前向传播 ---->( 查询 ，键 ， 值 ， 有效长度 )\n",
    "    def forward( self  ,  queries , keys , values , valid_lens ):\n",
    "        # 其中查询、键和值的形状为（批量大小、步数或词元序列长度、特征大小),\n",
    "        # queries ----> ( 批量大小 ， 查询个数 , num_hiddens )\n",
    "        # keys ----->( 批量大小 ，键值对个数 ， num_hiddens )\n",
    "        queries , keys = self.W_q( queries ) , self.W_k( keys )\n",
    "        #接下来要让实现加性注意力，我们让q与v的各个维度进行错开，实现(批量大小 ， 查询个数 ， 键值对个数 ， num_hiddens )\n",
    "        #queries( 批量大小 ， 查询个数 , num_hiddens ) ----> ( 批量大小 ， 查询个数 , 1 , num_hiddens )\n",
    "        #keys ( 批量大小 ，键值对个数 ， num_hiddens ) ----> ( 批量大小 ， 键值对个数 , 1 , num_hiddens )\n",
    "        #之后利用广播机制进行求和\n",
    "        features = queries.unsqueeze( 2 ) + keys.unsqueeze( 1 )\n",
    "        features = torch.tanh( features )\n",
    "        # self.w_v仅有一个输出 ，进行的操作是将\n",
    "        # ( 批量大小 ， 查询个数 ， 键值对个数 ， num_hiddens ) ----> ( 批量大小 ， 查询个数 ， 键值对个数 ， 1 )\n",
    "        # 之后消去最后那个维度\n",
    "        scores = self.W_v( features ).squeeze( -1 )#我们就得到了注意力分数( 批量大小 ， 查询个数 ， 键值对个数 )\n",
    "        #进行注意力汇聚，采用遮蔽softmax,我们应该还记得masked_softmax的基本处理方法，这里面当时写了X.size( 1 ),也\n",
    "        #就是说我们是对第一轴后的元素进行遮蔽，传入的的valid_lens往往是一阶tensor，那么这样的话是会对tensor进行广播\n",
    "        self.attention_weights = masked_softmax( scores , valid_lens  )\n",
    "        #values的形状---> ( 批量大小 ， 键值对个数  , 值的维度 )\n",
    "        #( 批量大小 ， 查询个数 ， 键值对个数 )bmm( 批量大小 ， 键值对个数 ， 值的维度 )---->\n",
    "        #( 批量大小 ， 查询个数 ， 值的维度 )\n",
    "        return torch.bmm( self.dropout( self.attention_weights ) , values ) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我认为的加性注意力的公式是这样的：\n",
    "$$\\alpha (q,k) = {W_v}^T\\tanh ({\\rm{q}}{{\\rm{W}}_a}{\\rm{ + }}k{W_k})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 我们现在进行测试：\n",
    "**注意：我们谨记查询(query),键(key),值(value)的形状都是：( batch_size , 步数或者词元序列长度 , 特征大小 )**\n",
    "1. query:( batch_size , 查询的个数 , 查询的维度 ) --->( 2 , 1  , 20  )\n",
    "2. key:( batch_size , 键值对的个数 ， 键的维度 ) --->( 2 , 10  , 2 )\n",
    "3. value:( batch_size , 键值对的个数 , 值的维度 ) --->( 2 , 10 , 4 )\n",
    "4. 最后得到的结果就是( batch_size , 查询的个数 , 值的维度 ) --->( 2 , 1 , 4 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 2.0000,  3.0000,  4.0000,  5.0000]],\n",
       "\n",
       "        [[10.0000, 11.0000, 12.0000, 13.0000]]], grad_fn=<BmmBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "queries , keys = torch.normal( 0 ,  1 , ( 2 ,1 ,20 )) , torch.ones( 2 ,10 ,2 )\n",
    "values = torch.arange( 40  , dtype= torch.float32 ).reshape( 1 , 10 , 4 ).repeat( 2 , 1 ,1 )\n",
    "valid_lens = torch.tensor( [ 2  ,6 ] )\n",
    "\n",
    "attention = AdditiveAttention( key_size= 2 , query_size= 20  , num_hiddens= 10 , dropout=0.1  )\n",
    "attention.eval()\n",
    "attention( queries , keys , values , valid_lens )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 缩放点积注意力"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基本的注意力分数就是${\\rm{a}}(q,k) = {q^T}k/\\sqrt d $,这里就是需要保证的q的q_size和键的k_size保持相等"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DotProductAttention( nn.Module ):\n",
    "    def __init__( self  , dropout , **kwargs ):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout( dropout )\n",
    "    # `queries` 的形状：(`batch_size`, 查询的个数, `d`)\n",
    "    # `keys` 的形状：(`batch_size`, “键－值”对的个数, `d`)\n",
    "    # `values` 的形状：(`batch_size`, “键－值”对的个数, 值的维度)\n",
    "    # `valid_lens` 的形状: (`batch_size`,) 或者 (`batch_size`, 查询的个数)\n",
    "    def forward( self , queries , keys , values , valid_lens  = None ):\n",
    "        d = queries.shape[-1]\n",
    "        #将keys的二轴与一轴进行转换 ---> ( `batch_size`, d , “键－值”对的个数 )\n",
    "        #(`batch_size`, 查询的个数, `d`)bmm( `batch_size`, d , “键－值”对的个数 )\n",
    "        #--->(`batch_size`, 查询的个数,  “键－值”对的个数 )\n",
    "        scores = torch.bmm( queries , keys.transpose( 1 , 2 ) ) /math.sqrt( d )\n",
    "\n",
    "        self.attention_weight = masked_softmax( scores , valid_lens )\n",
    "        # (`batch_size`, 查询的个数,  “键－值”对的个数 )bmm(  `batch_size`, “键－值”对的个数, 值的维度 )\n",
    "        # ----->(  batch_size , 查询的个数 ， 值的维度 )\n",
    "        return torch.bmm( self.dropout( self.attention_weight ) , values )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 2.0000,  3.0000,  4.0000,  5.0000]],\n",
       "\n",
       "        [[10.0000, 11.0000, 12.0000, 13.0000]]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries = torch.normal(0, 1, (2, 1, 2))\n",
    "attention = DotProductAttention(dropout=0.5)\n",
    "attention.eval()\n",
    "attention(queries, keys, values, valid_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"101.818906pt\" version=\"1.1\" viewBox=\"0 0 186.99575 101.818906\" width=\"186.99575pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2021-11-27T23:01:01.331877</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.3.4, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M -0 101.818906 \nL 186.99575 101.818906 \nL 186.99575 0 \nL -0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 34.240625 59.13 \nL 145.840625 59.13 \nL 145.840625 36.81 \nL 34.240625 36.81 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g clip-path=\"url(#p0bc85b6b38)\">\n    <image height=\"23\" id=\"imagec669fafff7\" transform=\"scale(1 -1)translate(0 -23)\" width=\"112\" x=\"34.240625\" xlink:href=\"data:image/png;base64,\niVBORw0KGgoAAAANSUhEUgAAAHAAAAAXCAYAAADTEcupAAAAeElEQVR4nO3YsQmAQBAF0X+aWp8tGFqDXZjZnC2YCV5uJnIcA/MKWBaGTbbcx/ZESZJxXnuv8NnQewH9Y0A4A8IZEM6AcAaEMyCcAeEMCGdAOAPClSVTk1/ofp0txurFC4QzIJwB4QwIZ0A4A8IZEM6AcAaEMyBcBc2IB/NA+hblAAAAAElFTkSuQmCC\" y=\"-36.13\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"mb6a85a68b9\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"39.820625\" xlink:href=\"#mb6a85a68b9\" y=\"59.13\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g transform=\"translate(36.639375 73.728437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"95.620625\" xlink:href=\"#mb6a85a68b9\" y=\"59.13\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 5 -->\n      <g transform=\"translate(92.439375 73.728437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_3\">\n     <!-- Keys -->\n     <g transform=\"translate(78.371094 87.406562)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 9.8125 72.90625 \nL 19.671875 72.90625 \nL 19.671875 42.09375 \nL 52.390625 72.90625 \nL 65.09375 72.90625 \nL 28.90625 38.921875 \nL 67.671875 0 \nL 54.6875 0 \nL 19.671875 35.109375 \nL 19.671875 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-75\"/>\n       <path d=\"M 56.203125 29.59375 \nL 56.203125 25.203125 \nL 14.890625 25.203125 \nQ 15.484375 15.921875 20.484375 11.0625 \nQ 25.484375 6.203125 34.421875 6.203125 \nQ 39.59375 6.203125 44.453125 7.46875 \nQ 49.3125 8.734375 54.109375 11.28125 \nL 54.109375 2.78125 \nQ 49.265625 0.734375 44.1875 -0.34375 \nQ 39.109375 -1.421875 33.890625 -1.421875 \nQ 20.796875 -1.421875 13.15625 6.1875 \nQ 5.515625 13.8125 5.515625 26.8125 \nQ 5.515625 40.234375 12.765625 48.109375 \nQ 20.015625 56 32.328125 56 \nQ 43.359375 56 49.78125 48.890625 \nQ 56.203125 41.796875 56.203125 29.59375 \nz\nM 47.21875 32.234375 \nQ 47.125 39.59375 43.09375 43.984375 \nQ 39.0625 48.390625 32.421875 48.390625 \nQ 24.90625 48.390625 20.390625 44.140625 \nQ 15.875 39.890625 15.1875 32.171875 \nz\n\" id=\"DejaVuSans-101\"/>\n       <path d=\"M 32.171875 -5.078125 \nQ 28.375 -14.84375 24.75 -17.8125 \nQ 21.140625 -20.796875 15.09375 -20.796875 \nL 7.90625 -20.796875 \nL 7.90625 -13.28125 \nL 13.1875 -13.28125 \nQ 16.890625 -13.28125 18.9375 -11.515625 \nQ 21 -9.765625 23.484375 -3.21875 \nL 25.09375 0.875 \nL 2.984375 54.6875 \nL 12.5 54.6875 \nL 29.59375 11.921875 \nL 46.6875 54.6875 \nL 56.203125 54.6875 \nz\n\" id=\"DejaVuSans-121\"/>\n       <path d=\"M 44.28125 53.078125 \nL 44.28125 44.578125 \nQ 40.484375 46.53125 36.375 47.5 \nQ 32.28125 48.484375 27.875 48.484375 \nQ 21.1875 48.484375 17.84375 46.4375 \nQ 14.5 44.390625 14.5 40.28125 \nQ 14.5 37.15625 16.890625 35.375 \nQ 19.28125 33.59375 26.515625 31.984375 \nL 29.59375 31.296875 \nQ 39.15625 29.25 43.1875 25.515625 \nQ 47.21875 21.78125 47.21875 15.09375 \nQ 47.21875 7.46875 41.1875 3.015625 \nQ 35.15625 -1.421875 24.609375 -1.421875 \nQ 20.21875 -1.421875 15.453125 -0.5625 \nQ 10.6875 0.296875 5.421875 2 \nL 5.421875 11.28125 \nQ 10.40625 8.6875 15.234375 7.390625 \nQ 20.0625 6.109375 24.8125 6.109375 \nQ 31.15625 6.109375 34.5625 8.28125 \nQ 37.984375 10.453125 37.984375 14.40625 \nQ 37.984375 18.0625 35.515625 20.015625 \nQ 33.0625 21.96875 24.703125 23.78125 \nL 21.578125 24.515625 \nQ 13.234375 26.265625 9.515625 29.90625 \nQ 5.8125 33.546875 5.8125 39.890625 \nQ 5.8125 47.609375 11.28125 51.796875 \nQ 16.75 56 26.8125 56 \nQ 31.78125 56 36.171875 55.265625 \nQ 40.578125 54.546875 44.28125 53.078125 \nz\n\" id=\"DejaVuSans-115\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-75\"/>\n      <use x=\"60.576172\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"122.099609\" xlink:href=\"#DejaVuSans-121\"/>\n      <use x=\"181.279297\" xlink:href=\"#DejaVuSans-115\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_3\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m7da11cf756\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"34.240625\" xlink:href=\"#m7da11cf756\" y=\"42.39\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 0 -->\n      <g transform=\"translate(20.878125 46.189219)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"34.240625\" xlink:href=\"#m7da11cf756\" y=\"53.55\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 1 -->\n      <g transform=\"translate(20.878125 57.349219)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-49\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_6\">\n     <!-- Queries -->\n     <g transform=\"translate(14.798437 67.277031)rotate(-90)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 39.40625 66.21875 \nQ 28.65625 66.21875 22.328125 58.203125 \nQ 16.015625 50.203125 16.015625 36.375 \nQ 16.015625 22.609375 22.328125 14.59375 \nQ 28.65625 6.59375 39.40625 6.59375 \nQ 50.140625 6.59375 56.421875 14.59375 \nQ 62.703125 22.609375 62.703125 36.375 \nQ 62.703125 50.203125 56.421875 58.203125 \nQ 50.140625 66.21875 39.40625 66.21875 \nz\nM 53.21875 1.3125 \nL 66.21875 -12.890625 \nL 54.296875 -12.890625 \nL 43.5 -1.21875 \nQ 41.890625 -1.3125 41.03125 -1.359375 \nQ 40.1875 -1.421875 39.40625 -1.421875 \nQ 24.03125 -1.421875 14.8125 8.859375 \nQ 5.609375 19.140625 5.609375 36.375 \nQ 5.609375 53.65625 14.8125 63.9375 \nQ 24.03125 74.21875 39.40625 74.21875 \nQ 54.734375 74.21875 63.90625 63.9375 \nQ 73.09375 53.65625 73.09375 36.375 \nQ 73.09375 23.6875 67.984375 14.640625 \nQ 62.890625 5.609375 53.21875 1.3125 \nz\n\" id=\"DejaVuSans-81\"/>\n       <path d=\"M 8.5 21.578125 \nL 8.5 54.6875 \nL 17.484375 54.6875 \nL 17.484375 21.921875 \nQ 17.484375 14.15625 20.5 10.265625 \nQ 23.53125 6.390625 29.59375 6.390625 \nQ 36.859375 6.390625 41.078125 11.03125 \nQ 45.3125 15.671875 45.3125 23.6875 \nL 45.3125 54.6875 \nL 54.296875 54.6875 \nL 54.296875 0 \nL 45.3125 0 \nL 45.3125 8.40625 \nQ 42.046875 3.421875 37.71875 1 \nQ 33.40625 -1.421875 27.6875 -1.421875 \nQ 18.265625 -1.421875 13.375 4.4375 \nQ 8.5 10.296875 8.5 21.578125 \nz\nM 31.109375 56 \nz\n\" id=\"DejaVuSans-117\"/>\n       <path d=\"M 41.109375 46.296875 \nQ 39.59375 47.171875 37.8125 47.578125 \nQ 36.03125 48 33.890625 48 \nQ 26.265625 48 22.1875 43.046875 \nQ 18.109375 38.09375 18.109375 28.8125 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 20.953125 51.171875 25.484375 53.578125 \nQ 30.03125 56 36.53125 56 \nQ 37.453125 56 38.578125 55.875 \nQ 39.703125 55.765625 41.0625 55.515625 \nz\n\" id=\"DejaVuSans-114\"/>\n       <path d=\"M 9.421875 54.6875 \nL 18.40625 54.6875 \nL 18.40625 0 \nL 9.421875 0 \nz\nM 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 64.59375 \nL 9.421875 64.59375 \nz\n\" id=\"DejaVuSans-105\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-81\"/>\n      <use x=\"78.710938\" xlink:href=\"#DejaVuSans-117\"/>\n      <use x=\"142.089844\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"203.613281\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"244.726562\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"272.509766\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"334.033203\" xlink:href=\"#DejaVuSans-115\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 34.240625 59.13 \nL 34.240625 36.81 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 145.840625 59.13 \nL 145.840625 36.81 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 34.240625 59.13 \nL 145.840625 59.13 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 34.240625 36.81 \nL 145.840625 36.81 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n  <g id=\"axes_2\">\n   <g id=\"patch_7\">\n    <path clip-path=\"url(#p842ae757a3)\" d=\"M 152.815625 88.74 \nL 152.815625 88.421484 \nL 152.815625 7.518516 \nL 152.815625 7.2 \nL 156.892625 7.2 \nL 156.892625 7.518516 \nL 156.892625 88.421484 \nL 156.892625 88.74 \nz\n\" style=\"fill:#ffffff;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.01;\"/>\n   </g>\n   <image height=\"82\" id=\"image6802b84cb0\" transform=\"scale(1 -1)translate(0 -82)\" width=\"4\" x=\"153\" xlink:href=\"data:image/png;base64,\niVBORw0KGgoAAAANSUhEUgAAAAQAAABSCAYAAABzJnWUAAAAnklEQVR4nJ2SOw5CMQwEjfTuf1QqChT/aMnskwxJl9F414ry6Nez7etclmE7qCLIAfRonABZLBy1o2FJEBzxxdD/M7R2NFh7EzpmHBhSm3z1H/YAaDEk1OfVBfCPCXCAEmPJppJRg1Fs0QyCYGhmw4jRIMjkSMgIjBBDapvGfrcrTYwDYINxsweAM8NpvAtgCeDIYqiMSIsYUrs/sdkHBN7KuQYqCJAAAAAASUVORK5CYII=\" y=\"-6\"/>\n   <g id=\"matplotlib.axis_3\"/>\n   <g id=\"matplotlib.axis_4\">\n    <g id=\"ytick_3\">\n     <g id=\"line2d_5\">\n      <defs>\n       <path d=\"M 0 0 \nL 3.5 0 \n\" id=\"m255ad2fe3a\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"156.892625\" xlink:href=\"#m255ad2fe3a\" y=\"88.74\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 0.0 -->\n      <g transform=\"translate(163.892625 92.539219)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 10.6875 12.40625 \nL 21 12.40625 \nL 21 0 \nL 10.6875 0 \nz\n\" id=\"DejaVuSans-46\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"156.892625\" xlink:href=\"#m255ad2fe3a\" y=\"56.124\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 0.2 -->\n      <g transform=\"translate(163.892625 59.923219)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_7\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"156.892625\" xlink:href=\"#m255ad2fe3a\" y=\"23.508\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 0.4 -->\n      <g transform=\"translate(163.892625 27.307219)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-52\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_8\">\n    <path d=\"M 152.815625 88.74 \nL 152.815625 88.421484 \nL 152.815625 7.518516 \nL 152.815625 7.2 \nL 156.892625 7.2 \nL 156.892625 7.518516 \nL 156.892625 88.421484 \nL 156.892625 88.74 \nz\n\" style=\"fill:none;stroke:#000000;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p0bc85b6b38\">\n   <rect height=\"22.32\" width=\"111.6\" x=\"34.240625\" y=\"36.81\"/>\n  </clipPath>\n  <clipPath id=\"p842ae757a3\">\n   <rect height=\"81.54\" width=\"4.077\" x=\"152.815625\" y=\"7.2\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "text/plain": [
       "<Figure size 180x180 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "d2l.show_heatmaps(attention.attention_weight.reshape((1, 1, 2, 10)),\n",
    "                  xlabel='Keys', ylabel='Queries')\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
