{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Residual( nn.Module ):\n",
    "    def __init__ ( self  , input_channels , num_channels , kernel_size = 3 , padding  =1 , \n",
    "                    use1x1cov = False,inputs_strides =1   ):\n",
    "        super().__init__()\n",
    "        self.cov1 =  nn.Conv2d( input_channels , num_channels , kernel_size=kernel_size , \n",
    "                            stride=inputs_strides , padding=padding , )#更改输出的特征长度，相当于将原来的特征图除以2\n",
    "        self.cov2 = nn.Conv2d( num_channels , num_channels , kernel_size= kernel_size , \n",
    "                            stride=1 , padding= padding )\n",
    "        if use1x1cov :\n",
    "            self.cov3 = nn.Conv2d( input_channels , num_channels ,kernel_size=1,stride=inputs_strides )\n",
    "            #如果更改了特征图的长度，那么就需要对输入x也更改特征图的长度，这里使用1*1的卷积核进行更改\n",
    "        else:\n",
    "            self.cov3 = None\n",
    "        self.bn1 = nn.BatchNorm2d( num_channels )\n",
    "        self.bn2 = nn.BatchNorm2d( num_channels )\n",
    "    \n",
    "    def forward( self , X ):\n",
    "        Y = F.relu( self.bn1( self.cov1( X )  ))\n",
    "        Y = self.bn2( self.cov2( Y ) )\n",
    "        if self.cov3:\n",
    "            X = self.cov3( X )\n",
    "        Y += X \n",
    "        return F.relu( Y )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 3, 3])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blk = Residual( 3,3 , use1x1cov=True  , inputs_strides= 2 )\n",
    "x = torch.rand( 4 ,3 , 6,6)\n",
    "x = blk( x )\n",
    "x.shape#更改长度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 6, 6, 6])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blk = Residual( 3,6 ,use1x1cov=True )\n",
    "x = torch.rand( 4 ,3 , 6,6)\n",
    "x = blk( x )\n",
    "x.shape#更改通道数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet_block( input_channels , num_channels , num_residuals , first_block = False ):\n",
    "    #我们将这个Residual块添加到\n",
    "    blk = []\n",
    "    for i in range( num_residuals ):\n",
    "        if i == 0 and not first_block:\n",
    "            #如果是非第一个模块的第一个residucl块的话，就将改变通道数并且减半特征图大小的残差模块添加进去,conv_block\n",
    "            blk.append( Residual( input_channels , num_channels  , use1x1cov=True , inputs_strides=2 ) )\n",
    "        else:#如果是第一个模块的第一个residual块，就添加到block当中，这里是承接maxpooling2d的第一个残差块\n",
    "            #如果不是非第一个模块的非第一个residucl块的话，就将当前不增加通道数的residucl块添加进去。identity_block\n",
    "            blk.append( Residual( num_channels , num_channels ) )\n",
    "    return blk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "b1 = nn.Sequential( nn.Conv2d( 1, 64 , kernel_size=7 , stride=2 , padding=3 ) , nn.BatchNorm2d( 64 ), nn.ReLU(),\n",
    "                        nn.MaxPool2d( kernel_size=3 , stride= 2 , padding=1 ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "b2 = nn.Sequential( *resnet_block( 64 , 64 , 2 , first_block=True ) )#每个模块包含两个残差块并且因为在maxplooing的时候我们实现了减半的操作，\n",
    "                                                                    #所以这里的第一个残差块就不需要减半了,也就是说直接执行else语句添加的是\n",
    "                                                                    #两个相同的输入输出通道\n",
    "b3 = nn.Sequential( *resnet_block( 64 , 128 , 2 ) )\n",
    "b4 = nn.Sequential( *resnet_block( 128 , 256 , 2) )\n",
    "b5 = nn.Sequential( *resnet_block( 256 ,512 , 2 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential(b1, b2, b3,b4,b5,\n",
    "                    nn.AdaptiveAvgPool2d((1,1)),\n",
    "                    nn.Flatten(), nn.Linear(512, 10)  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential output_size: torch.Size([1, 64, 56, 56])\n",
      "Sequential output_size: torch.Size([1, 64, 56, 56])\n",
      "Sequential output_size: torch.Size([1, 128, 28, 28])\n",
      "Sequential output_size: torch.Size([1, 256, 14, 14])\n",
      "Sequential output_size: torch.Size([1, 512, 7, 7])\n",
      "AdaptiveAvgPool2d output_size: torch.Size([1, 512, 1, 1])\n",
      "Flatten output_size: torch.Size([1, 512])\n",
      "Linear output_size: torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand( 1,  1 ,224 ,224 )\n",
    "for layer in net:\n",
    "    x = layer( x )\n",
    "    print( layer.__class__.__name__,'output_size:',x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on cuda:3\n",
      "loss 0.449, train acc 0.839 \n",
      "test_acc is 0.880\n",
      "loss 0.256, train acc 0.907 \n",
      "test_acc is 0.847\n",
      "loss 0.211, train acc 0.923 \n",
      "test_acc is 0.903\n",
      "loss 0.184, train acc 0.932 \n",
      "test_acc is 0.914\n",
      "loss 0.157, train acc 0.942 \n",
      "test_acc is 0.893\n",
      "loss 0.137, train acc 0.949 \n",
      "test_acc is 0.894\n",
      "loss 0.115, train acc 0.958 \n",
      "test_acc is 0.923\n",
      "loss 0.095, train acc 0.965 \n",
      "test_acc is 0.928\n",
      "loss 0.075, train acc 0.973 \n",
      "test_acc is 0.921\n",
      "loss 0.054, train acc 0.981 \n",
      "test_acc is 0.917\n",
      "loss 0.048, train acc 0.983 \n",
      "test_acc is 0.928\n",
      "loss 0.034, train acc 0.988 \n",
      "test_acc is 0.916\n",
      "loss 0.036, train acc 0.987 \n",
      "test_acc is 0.926\n",
      "loss 0.023, train acc 0.992 \n",
      "test_acc is 0.926\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-5011a1c31d0b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.001\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtrain_iter\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mtest_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md2l\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_data_fashion_mnist\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mresize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m224\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0md2l\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_no_image\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mnet\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mtrain_iter\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mtest_iter\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0md2l\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtry_gpu\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mnum_print\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/d2l/torch.py\u001b[0m in \u001b[0;36mtrain_no_image\u001b[0;34m(net, train_iter, test_iter, num_epochs, lr, device, num_print)\u001b[0m\n\u001b[1;32m    540\u001b[0m             \u001b[0my_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m             \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m             \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lr , num_epochs , batch_size = 0.001 , 20 , 128\n",
    "train_iter , test_iter = d2l.load_data_fashion_mnist( batch_size=batch_size , resize = 224 )\n",
    "d2l.train_no_image( net , train_iter , test_iter , num_epochs , lr , d2l.try_gpu( i=3 ) , num_print=1 )"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4e3237dc568d8c012c4be0ad63f07931f6897ff43d3b726284ebc51bc8854128"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('d2l': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
