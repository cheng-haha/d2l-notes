{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "import torch\r\n",
    "from torch import nn"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "p = torch.arange(0,16 ).reshape(4,4)\r\n",
    "params = [ p , p , p ]\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "sum( torch.sum( (p ** 2) ) for p in params  )"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(3720)"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def grad_clipping(net, theta):  #@save\r\n",
    "    \"\"\"裁剪梯度。\"\"\"\r\n",
    "    if isinstance(net, nn.Module):\r\n",
    "        params = [p for p in net.parameters() if p.requires_grad]\r\n",
    "    else:\r\n",
    "        params = net.params\r\n",
    "    \r\n",
    "    norm = torch.sqrt(sum(  torch.sum( (p.grad ** 2) ) for p in params ) )\r\n",
    "    if norm > theta:\r\n",
    "        for param in params:\r\n",
    "            param.grad[:] *= theta / norm"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "x =torch.normal( 0 ,1 ,( 2,2 ))\r\n",
    "y= torch.normal( 0 ,1 ,( 2,2 ))\r\n",
    "loss = nn.CrossEntropyLoss( )\r\n",
    "x"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[-0.6130,  0.0716],\n",
       "        [-1.6206, -0.5793]])"
      ]
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "source": [
    "loss = nn.CrossEntropyLoss()\r\n",
    "input = torch.ones((3, 5), requires_grad=True)\r\n",
    "target = torch.empty(3, dtype=torch.long).random_(5)\r\n",
    "output = loss(input, target).mean()#这里加了mean和不加是一样的，因为默认情况下CrossEntropyLoss()就已经对批次数据进行了平均\r\n",
    "output.backward()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "source": [
    "output"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(1.6094, grad_fn=<MeanBackward0>)"
      ]
     },
     "metadata": {},
     "execution_count": 43
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit ('d2l': conda)"
  },
  "interpreter": {
   "hash": "4e3237dc568d8c012c4be0ad63f07931f6897ff43d3b726284ebc51bc8854128"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}